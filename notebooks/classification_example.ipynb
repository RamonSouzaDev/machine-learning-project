{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Study: Classification Example\n",
    "\n",
    "This notebook demonstrates how to use the Machine Learning Study project for a classification task using the Iris dataset.\n",
    "\n",
    "## Overview\n",
    "\n",
    "We'll cover:\n",
    "1. Setting up the environment\n",
    "2. Loading and exploring data\n",
    "3. Running a complete ML experiment\n",
    "4. Analyzing results\n",
    "5. Experiment tracking with MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the src directory to Python path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Import our ML study modules\n",
    "from machine_learning_study.pipelines import run_experiment\n",
    "from machine_learning_study.data.loader import DataLoader\n",
    "from machine_learning_study.models.classifier import Classifier\n",
    "from machine_learning_study.models.evaluator import ModelEvaluator\n",
    "from machine_learning_study.utils.experiment_tracker import ExperimentTracker\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "data_loader = DataLoader('data')\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris_data = data_loader.load_sklearn_dataset('iris')\n",
    "\n",
    "# Create DataFrame for easier manipulation\n",
    "df = pd.DataFrame(\n",
    "    iris_data['data'], \n",
    "    columns=iris_data['feature_names']\n",
    ")\n",
    "df['target'] = iris_data['target']\n",
    "df['species'] = df['target'].map(dict(enumerate(iris_data['target_names'])))\n",
    "\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data exploration\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nClass distribution:\")\n",
    "print(df['species'].value_counts())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "fig.suptitle('Iris Dataset Feature Distributions', fontsize=16)\n",
    "\n",
    "# Plot histograms for each feature\n",
    "features = iris_data['feature_names']\n",
    "for i, feature in enumerate(features):\n",
    "    ax = axes[i//2, i%2]\n",
    "    for species in df['species'].unique():\n",
    "        species_data = df[df['species'] == species][feature]\n",
    "        ax.hist(species_data, alpha=0.7, label=species, bins=15)\n",
    "    ax.set_title(f'{feature} Distribution')\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pair plot to see feature relationships\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.pairplot(df, hue='species', diag_kind='kde', height=2)\n",
    "plt.suptitle('Pair Plot of Iris Features', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running a Complete ML Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experiment configuration\n",
    "config = {\n",
    "    \"task\": \"classification\",\n",
    "    \"target_column\": \"target\",\n",
    "    \"random_state\": 42,\n",
    "    \"data\": {\n",
    "        \"sklearn_dataset\": \"iris\"\n",
    "    },\n",
    "    \"preprocessing\": {\n",
    "        \"scaling\": {\n",
    "            \"method\": \"standard\"\n",
    "        }\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"selection\": {\n",
    "            \"method\": \"mutual_info\",\n",
    "            \"k\": 4\n",
    "        }\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"type\": \"random_forest\",\n",
    "        \"parameters\": {\n",
    "            \"n_estimators\": 100,\n",
    "            \"max_depth\": 10,\n",
    "            \"random_state\": 42\n",
    "        }\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"test_size\": 0.2,\n",
    "        \"stratify\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save configuration to file\n",
    "import yaml\n",
    "config_path = 'notebooks/iris_experiment_config.yaml'\n",
    "with open(config_path, 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"Configuration saved to:\", config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "print(\"üöÄ Running ML experiment...\")\n",
    "results = run_experiment(\"iris_classification\", config_path)\n",
    "print(\"‚úÖ Experiment completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyzing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display evaluation metrics\n",
    "metrics = results['evaluation_results']\n",
    "cv_results = results['cross_validation_results']\n",
    "\n",
    "print(\"üìä Model Performance Metrics:\")\n",
    "print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score: {metrics['f1_score']:.4f}\")\n",
    "\n",
    "print(\"\\nüìà Cross-Validation Results:\")\n",
    "print(f\"Mean CV Score: {cv_results['metrics']['mean_score']:.4f}\")\n",
    "print(f\"Std CV Score: {cv_results['metrics']['std_score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions and analyze them\n",
    "predictions = results['pipeline_results']['predictions']\n",
    "y_test = results['pipeline_results']['splits']['y_test']\n",
    "\n",
    "# Create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=iris_data['target_names'],\n",
    "            yticklabels=iris_data['target_names'])\n",
    "plt.title('Confusion Matrix - Iris Classification')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "model = results['pipeline_results']['model']\n",
    "feature_importance = model.get_feature_importance()\n",
    "\n",
    "if feature_importance is not None:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    features = iris_data['feature_names']\n",
    "    plt.barh(features, feature_importance)\n",
    "    plt.title('Feature Importance - Random Forest')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Feature importance not available for this model type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment Tracking with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize experiment tracker\n",
    "tracker = ExperimentTracker(experiment_name=\"iris_classification_study\")\n",
    "\n",
    "# Log the experiment results\n",
    "with tracker.start_run(run_name=\"notebook_iris_experiment\"):\n",
    "    # Log parameters\n",
    "    tracker.log_parameters({\n",
    "        \"model_type\": \"random_forest\",\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 10,\n",
    "        \"test_size\": 0.2\n",
    "    })\n",
    "    \n",
    "    # Log metrics\n",
    "    tracker.log_metrics(metrics)\n",
    "    \n",
    "    # Log model\n",
    "    tracker.log_model(model.model, \"random_forest_model\")\n",
    "    \n",
    "    # Log additional information\n",
    "    tracker.log_dict({\n",
    "        \"dataset\": \"iris\",\n",
    "        \"cv_folds\": 5,\n",
    "        \"experiment_type\": \"classification_example\"\n",
    "    }, \"experiment_metadata.json\")\n",
    "\n",
    "print(\"‚úÖ Experiment logged to MLflow!\")\n",
    "print(\"View results at: http://localhost:5000 (if MLflow server is running)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different models\n",
    "models_to_test = ['random_forest', 'gradient_boosting', 'svm', 'knn']\n",
    "model_results = {}\n",
    "\n",
    "X = df.drop(['target', 'species'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "evaluator = ModelEvaluator(task='classification')\n",
    "\n",
    "for model_type in models_to_test:\n",
    "    print(f\"\\nüîç Testing {model_type}...\")\n",
    "    \n",
    "    # Create and train model\n",
    "    model = Classifier(model_type=model_type, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Cross-validate\n",
    "    cv_results = model.cross_validate(X, y, cv=5)\n",
    "    \n",
    "    model_results[model_type] = {\n",
    "        'cv_mean': cv_results['mean_score'],\n",
    "        'cv_std': cv_results['std_score']\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_type}: {cv_results['mean_score']:.4f} (+/- {cv_results['std_score']:.4f})\")\n",
    "\n",
    "# Visualize comparison\n",
    "model_names = list(model_results.keys())\n",
    "cv_means = [model_results[m]['cv_mean'] for m in model_names]\n",
    "cv_stds = [model_results[m]['cv_std'] for m in model_names]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(model_names, cv_means, yerr=cv_stds, capsize=5)\n",
    "plt.title('Model Comparison - Cross-Validation Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0.8, 1.0)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, mean in zip(bars, cv_means):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{mean:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Data Loading & Exploration**: Loading sklearn datasets and basic EDA\n",
    "2. **ML Pipeline Execution**: Running complete experiments with configuration\n",
    "3. **Result Analysis**: Evaluating model performance and visualizing results\n",
    "4. **Experiment Tracking**: Logging experiments with MLflow\n",
    "5. **Model Comparison**: Comparing different algorithms\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- The project provides a clean, modular approach to ML experimentation\n",
    "- Experiment tracking ensures reproducibility and comparison\n",
    "- Configuration-driven approach makes it easy to modify experiments\n",
    "- Integration with MLflow provides enterprise-grade experiment management\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Try different datasets and configurations\n",
    "2. Experiment with feature engineering techniques\n",
    "3. Explore hyperparameter optimization with Optuna\n",
    "4. Deploy models using the included Docker setup\n",
    "\n",
    "Happy learning! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}